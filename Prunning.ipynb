{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -q tensorflow-model-optimization\n",
    "#! pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1803.03635\n",
    "\n",
    "https://www.youtube.com/watch?v=0VH1Lim8gL8&feature=youtu.be&t=2760"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "import os\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# import tensorflow_addons as tfa\n",
    "# from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "# import tensorflow.keras\n",
    "# import matplotlib.pyplot as plt\n",
    "# from IPython.display import clear_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# from tensorflow.keras.models import Model\n",
    "# import tensorflow.keras as keras\n",
    "# from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seeds(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    np.random.seed(seed)\n",
    "    rn.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    # session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    # sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "    # K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21a1ba1a316bb0a6f5d8f85d86b3ba2307f125af"
   },
   "source": [
    "# Cargo datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "7720949cd52b1de2d7687500e3d121f30bc80f82"
   },
   "outputs": [],
   "source": [
    "X = np.load(folder+'train_images.npy').reshape(-1, 784)/255\n",
    "y = np.loadtxt(folder+'train_labels.csv', delimiter=',', skiprows=1).reshape(-1, 1)\n",
    "X_test = np.load(folder+'test_images.npy').reshape(-1, 784)/255\n",
    "y_test = pd.read_csv(folder+'test_labels.csv')['Category'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f987cc26468c6d17e63a953827437abb66da4383"
   },
   "source": [
    "# Red neuronal b√°sica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, lr=0.001):\n",
    "    optim = optimizers.Adam(lr=lr)\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer=optim, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "6e998b9f383e967a21d0ee6a89dfc8fec72d7f80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden_1 (Dense)             (None, 1568)              1230880   \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "hidden_2 (Dense)             (None, 784)               1230096   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "Salida (Dense)               (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 2,468,826\n",
      "Trainable params: 2,468,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model(compile_model_flag=True, lr=0.001):\n",
    "    input_dim=784\n",
    "    output_size = 10\n",
    "    # Creo el modelo\n",
    "    model = Sequential()\n",
    "    model.add(Dense(784*2, activation='linear', name='hidden_1', input_dim=input_dim))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(784, activation='linear', name='hidden_2'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(output_size, name='Salida'))\n",
    "    model.add(Activation('softmax'))\n",
    "    if compile_model_flag:\n",
    "        compile_model(model, lr=lr)\n",
    "    return model\n",
    "model = get_model()\n",
    "# model.save_weights('random-init.hdf5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(filename):\n",
    "    return [\n",
    "        ReduceLROnPlateau(monitor='val_accuracy', mode='max', factor=np.sqrt(0.1), patience=10, verbose=1, min_lr=1e-4),\n",
    "        ModelCheckpoint(filepath=filename,  verbose=1, save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "bac8f7a2f7fb157ec21445359a6f316073515ff4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/96\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.84922, saving model to mlp.mnist.first_train.hdf5\n",
      "200/200 - 1s - loss: 0.5000 - accuracy: 0.8201 - val_loss: 0.4116 - val_accuracy: 0.8492 - lr: 0.0010\n",
      "Epoch 2/96\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.84922 to 0.86811, saving model to mlp.mnist.first_train.hdf5\n",
      "200/200 - 1s - loss: 0.3492 - accuracy: 0.8725 - val_loss: 0.3592 - val_accuracy: 0.8681 - lr: 0.0010\n",
      "Epoch 3/96\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.86811\n",
      "200/200 - 0s - loss: 0.3132 - accuracy: 0.8825 - val_loss: 0.3740 - val_accuracy: 0.8577 - lr: 0.0010\n",
      "Epoch 4/96\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.86811 to 0.88033, saving model to mlp.mnist.first_train.hdf5\n",
      "200/200 - 1s - loss: 0.2932 - accuracy: 0.8898 - val_loss: 0.3202 - val_accuracy: 0.8803 - lr: 0.0010\n",
      "Epoch 5/96\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.88033\n",
      "200/200 - 1s - loss: 0.2721 - accuracy: 0.8989 - val_loss: 0.3401 - val_accuracy: 0.8731 - lr: 0.0010\n",
      "Epoch 6/96\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.88033 to 0.88722, saving model to mlp.mnist.first_train.hdf5\n",
      "200/200 - 1s - loss: 0.2601 - accuracy: 0.9032 - val_loss: 0.3067 - val_accuracy: 0.8872 - lr: 0.0010\n",
      "Epoch 7/96\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.88722\n",
      "200/200 - 0s - loss: 0.2389 - accuracy: 0.9106 - val_loss: 0.3216 - val_accuracy: 0.8844 - lr: 0.0010\n",
      "Epoch 8/96\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.88722\n",
      "200/200 - 1s - loss: 0.2336 - accuracy: 0.9121 - val_loss: 0.3184 - val_accuracy: 0.8850 - lr: 0.0010\n",
      "Epoch 9/96\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.88722 to 0.89144, saving model to mlp.mnist.first_train.hdf5\n",
      "200/200 - 0s - loss: 0.2188 - accuracy: 0.9171 - val_loss: 0.3109 - val_accuracy: 0.8914 - lr: 0.0010\n",
      "Epoch 10/96\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.89144\n",
      "200/200 - 0s - loss: 0.2169 - accuracy: 0.9173 - val_loss: 0.3452 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Epoch 11/96\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.89144 to 0.89322, saving model to mlp.mnist.first_train.hdf5\n",
      "200/200 - 1s - loss: 0.1992 - accuracy: 0.9254 - val_loss: 0.3067 - val_accuracy: 0.8932 - lr: 0.0010\n",
      "Epoch 12/96\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.89322\n",
      "200/200 - 0s - loss: 0.1959 - accuracy: 0.9258 - val_loss: 0.3446 - val_accuracy: 0.8840 - lr: 0.0010\n",
      "Epoch 13/96\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.89322\n",
      "200/200 - 1s - loss: 0.1830 - accuracy: 0.9305 - val_loss: 0.3188 - val_accuracy: 0.8914 - lr: 0.0010\n",
      "Epoch 14/96\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.89322\n",
      "200/200 - 1s - loss: 0.1783 - accuracy: 0.9323 - val_loss: 0.3091 - val_accuracy: 0.8922 - lr: 0.0010\n",
      "Epoch 15/96\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.89322\n",
      "200/200 - 0s - loss: 0.1667 - accuracy: 0.9363 - val_loss: 0.3331 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Epoch 16/96\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.89322\n",
      "200/200 - 0s - loss: 0.1664 - accuracy: 0.9367 - val_loss: 0.3296 - val_accuracy: 0.8932 - lr: 0.0010\n",
      "Epoch 17/96\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.89322 to 0.89789, saving model to mlp.mnist.first_train.hdf5\n",
      "200/200 - 1s - loss: 0.1554 - accuracy: 0.9403 - val_loss: 0.3253 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Epoch 18/96\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.89789\n",
      "200/200 - 1s - loss: 0.1507 - accuracy: 0.9419 - val_loss: 0.3503 - val_accuracy: 0.8871 - lr: 0.0010\n",
      "Epoch 19/96\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.89789\n",
      "200/200 - 1s - loss: 0.1437 - accuracy: 0.9452 - val_loss: 0.3680 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Epoch 20/96\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.89789\n",
      "200/200 - 0s - loss: 0.1371 - accuracy: 0.9468 - val_loss: 0.3463 - val_accuracy: 0.8934 - lr: 0.0010\n",
      "Epoch 21/96\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.89789\n",
      "200/200 - 1s - loss: 0.1398 - accuracy: 0.9465 - val_loss: 0.3305 - val_accuracy: 0.8968 - lr: 0.0010\n",
      "Epoch 22/96\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.89789\n",
      "200/200 - 0s - loss: 0.1251 - accuracy: 0.9513 - val_loss: 0.3559 - val_accuracy: 0.8963 - lr: 0.0010\n",
      "Epoch 23/96\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.89789\n",
      "200/200 - 0s - loss: 0.1206 - accuracy: 0.9525 - val_loss: 0.3678 - val_accuracy: 0.8908 - lr: 0.0010\n",
      "Epoch 24/96\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.89789\n",
      "200/200 - 0s - loss: 0.1253 - accuracy: 0.9515 - val_loss: 0.3554 - val_accuracy: 0.8961 - lr: 0.0010\n",
      "Epoch 25/96\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.89789\n",
      "200/200 - 1s - loss: 0.1174 - accuracy: 0.9554 - val_loss: 0.3583 - val_accuracy: 0.8946 - lr: 0.0010\n",
      "Epoch 26/96\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.89789\n",
      "200/200 - 0s - loss: 0.1063 - accuracy: 0.9591 - val_loss: 0.3879 - val_accuracy: 0.8928 - lr: 0.0010\n",
      "Epoch 27/96\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00031622778103685084.\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.89789\n",
      "200/200 - 1s - loss: 0.1033 - accuracy: 0.9599 - val_loss: 0.3935 - val_accuracy: 0.8946 - lr: 0.0010\n",
      "Epoch 28/96\n",
      "\n",
      "Epoch 00028: val_accuracy improved from 0.89789 to 0.90611, saving model to mlp.mnist.first_train.hdf5\n",
      "200/200 - 0s - loss: 0.0662 - accuracy: 0.9754 - val_loss: 0.3745 - val_accuracy: 0.9061 - lr: 3.1623e-04\n",
      "Epoch 29/96\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0586 - accuracy: 0.9788 - val_loss: 0.3937 - val_accuracy: 0.9046 - lr: 3.1623e-04\n",
      "Epoch 30/96\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0543 - accuracy: 0.9796 - val_loss: 0.3953 - val_accuracy: 0.9048 - lr: 3.1623e-04\n",
      "Epoch 31/96\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0497 - accuracy: 0.9824 - val_loss: 0.4180 - val_accuracy: 0.9040 - lr: 3.1623e-04\n",
      "Epoch 32/96\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0486 - accuracy: 0.9827 - val_loss: 0.4144 - val_accuracy: 0.9039 - lr: 3.1623e-04\n",
      "Epoch 33/96\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0465 - accuracy: 0.9829 - val_loss: 0.4339 - val_accuracy: 0.9009 - lr: 3.1623e-04\n",
      "Epoch 34/96\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0432 - accuracy: 0.9851 - val_loss: 0.4398 - val_accuracy: 0.9049 - lr: 3.1623e-04\n",
      "Epoch 35/96\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0403 - accuracy: 0.9863 - val_loss: 0.4474 - val_accuracy: 0.9037 - lr: 3.1623e-04\n",
      "Epoch 36/96\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0387 - accuracy: 0.9864 - val_loss: 0.4471 - val_accuracy: 0.9028 - lr: 3.1623e-04\n",
      "Epoch 37/96\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0362 - accuracy: 0.9875 - val_loss: 0.4533 - val_accuracy: 0.9027 - lr: 3.1623e-04\n",
      "Epoch 38/96\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00010000000639606199.\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0333 - accuracy: 0.9885 - val_loss: 0.4706 - val_accuracy: 0.9022 - lr: 3.1623e-04\n",
      "Epoch 39/96\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0242 - accuracy: 0.9931 - val_loss: 0.4782 - val_accuracy: 0.9042 - lr: 1.0000e-04\n",
      "Epoch 40/96\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0218 - accuracy: 0.9940 - val_loss: 0.4867 - val_accuracy: 0.9051 - lr: 1.0000e-04\n",
      "Epoch 41/96\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0210 - accuracy: 0.9942 - val_loss: 0.4845 - val_accuracy: 0.9061 - lr: 1.0000e-04\n",
      "Epoch 42/96\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0197 - accuracy: 0.9950 - val_loss: 0.4994 - val_accuracy: 0.9050 - lr: 1.0000e-04\n",
      "Epoch 43/96\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0192 - accuracy: 0.9952 - val_loss: 0.5067 - val_accuracy: 0.9039 - lr: 1.0000e-04\n",
      "Epoch 44/96\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0187 - accuracy: 0.9953 - val_loss: 0.5075 - val_accuracy: 0.9046 - lr: 1.0000e-04\n",
      "Epoch 45/96\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0180 - accuracy: 0.9956 - val_loss: 0.5100 - val_accuracy: 0.9056 - lr: 1.0000e-04\n",
      "Epoch 46/96\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0173 - accuracy: 0.9957 - val_loss: 0.5195 - val_accuracy: 0.9044 - lr: 1.0000e-04\n",
      "Epoch 47/96\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0169 - accuracy: 0.9958 - val_loss: 0.5279 - val_accuracy: 0.9039 - lr: 1.0000e-04\n",
      "Epoch 48/96\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0158 - accuracy: 0.9965 - val_loss: 0.5336 - val_accuracy: 0.9054 - lr: 1.0000e-04\n",
      "Epoch 49/96\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0151 - accuracy: 0.9966 - val_loss: 0.5423 - val_accuracy: 0.9026 - lr: 1.0000e-04\n",
      "Epoch 50/96\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0144 - accuracy: 0.9967 - val_loss: 0.5425 - val_accuracy: 0.9032 - lr: 1.0000e-04\n",
      "Epoch 51/96\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0137 - accuracy: 0.9971 - val_loss: 0.5490 - val_accuracy: 0.9027 - lr: 1.0000e-04\n",
      "Epoch 52/96\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0130 - accuracy: 0.9975 - val_loss: 0.5570 - val_accuracy: 0.9017 - lr: 1.0000e-04\n",
      "Epoch 53/96\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0128 - accuracy: 0.9971 - val_loss: 0.5586 - val_accuracy: 0.9038 - lr: 1.0000e-04\n",
      "Epoch 54/96\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0121 - accuracy: 0.9973 - val_loss: 0.5760 - val_accuracy: 0.9031 - lr: 1.0000e-04\n",
      "Epoch 55/96\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0116 - accuracy: 0.9977 - val_loss: 0.5821 - val_accuracy: 0.9044 - lr: 1.0000e-04\n",
      "Epoch 56/96\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0108 - accuracy: 0.9981 - val_loss: 0.5898 - val_accuracy: 0.9028 - lr: 1.0000e-04\n",
      "Epoch 57/96\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0105 - accuracy: 0.9980 - val_loss: 0.5941 - val_accuracy: 0.9023 - lr: 1.0000e-04\n",
      "Epoch 58/96\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0098 - accuracy: 0.9982 - val_loss: 0.6011 - val_accuracy: 0.9036 - lr: 1.0000e-04\n",
      "Epoch 59/96\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0095 - accuracy: 0.9983 - val_loss: 0.5990 - val_accuracy: 0.9032 - lr: 1.0000e-04\n",
      "Epoch 60/96\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0092 - accuracy: 0.9982 - val_loss: 0.6120 - val_accuracy: 0.9021 - lr: 1.0000e-04\n",
      "Epoch 61/96\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0082 - accuracy: 0.9987 - val_loss: 0.6240 - val_accuracy: 0.9042 - lr: 1.0000e-04\n",
      "Epoch 62/96\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0080 - accuracy: 0.9988 - val_loss: 0.6288 - val_accuracy: 0.9030 - lr: 1.0000e-04\n",
      "Epoch 63/96\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0075 - accuracy: 0.9988 - val_loss: 0.6343 - val_accuracy: 0.9016 - lr: 1.0000e-04\n",
      "Epoch 64/96\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0074 - accuracy: 0.9988 - val_loss: 0.6487 - val_accuracy: 0.9012 - lr: 1.0000e-04\n",
      "Epoch 65/96\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0070 - accuracy: 0.9990 - val_loss: 0.6438 - val_accuracy: 0.9041 - lr: 1.0000e-04\n",
      "Epoch 66/96\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0065 - accuracy: 0.9990 - val_loss: 0.6553 - val_accuracy: 0.9024 - lr: 1.0000e-04\n",
      "Epoch 67/96\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.6684 - val_accuracy: 0.9018 - lr: 1.0000e-04\n",
      "Epoch 68/96\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0059 - accuracy: 0.9992 - val_loss: 0.6690 - val_accuracy: 0.9033 - lr: 1.0000e-04\n",
      "Epoch 69/96\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0055 - accuracy: 0.9993 - val_loss: 0.6851 - val_accuracy: 0.9041 - lr: 1.0000e-04\n",
      "Epoch 70/96\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0052 - accuracy: 0.9994 - val_loss: 0.6804 - val_accuracy: 0.9023 - lr: 1.0000e-04\n",
      "Epoch 71/96\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0054 - accuracy: 0.9994 - val_loss: 0.6859 - val_accuracy: 0.9030 - lr: 1.0000e-04\n",
      "Epoch 72/96\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0048 - accuracy: 0.9994 - val_loss: 0.6991 - val_accuracy: 0.9027 - lr: 1.0000e-04\n",
      "Epoch 73/96\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0049 - accuracy: 0.9993 - val_loss: 0.7112 - val_accuracy: 0.9023 - lr: 1.0000e-04\n",
      "Epoch 74/96\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0045 - accuracy: 0.9995 - val_loss: 0.7101 - val_accuracy: 0.9029 - lr: 1.0000e-04\n",
      "Epoch 75/96\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0041 - accuracy: 0.9996 - val_loss: 0.7121 - val_accuracy: 0.9033 - lr: 1.0000e-04\n",
      "Epoch 76/96\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0040 - accuracy: 0.9996 - val_loss: 0.7288 - val_accuracy: 0.9018 - lr: 1.0000e-04\n",
      "Epoch 77/96\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0036 - accuracy: 0.9996 - val_loss: 0.7301 - val_accuracy: 0.9022 - lr: 1.0000e-04\n",
      "Epoch 78/96\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0033 - accuracy: 0.9997 - val_loss: 0.7364 - val_accuracy: 0.9013 - lr: 1.0000e-04\n",
      "Epoch 79/96\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0033 - accuracy: 0.9997 - val_loss: 0.7395 - val_accuracy: 0.9036 - lr: 1.0000e-04\n",
      "Epoch 80/96\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0032 - accuracy: 0.9997 - val_loss: 0.7435 - val_accuracy: 0.9001 - lr: 1.0000e-04\n",
      "Epoch 81/96\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0032 - accuracy: 0.9997 - val_loss: 0.7527 - val_accuracy: 0.9020 - lr: 1.0000e-04\n",
      "Epoch 82/96\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0030 - accuracy: 0.9997 - val_loss: 0.7646 - val_accuracy: 0.9024 - lr: 1.0000e-04\n",
      "Epoch 83/96\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0025 - accuracy: 0.9999 - val_loss: 0.7698 - val_accuracy: 0.9021 - lr: 1.0000e-04\n",
      "Epoch 84/96\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0028 - accuracy: 0.9997 - val_loss: 0.7754 - val_accuracy: 0.9001 - lr: 1.0000e-04\n",
      "Epoch 85/96\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.7861 - val_accuracy: 0.8988 - lr: 1.0000e-04\n",
      "Epoch 86/96\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0028 - accuracy: 0.9997 - val_loss: 0.7891 - val_accuracy: 0.9002 - lr: 1.0000e-04\n",
      "Epoch 87/96\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0022 - accuracy: 0.9999 - val_loss: 0.7981 - val_accuracy: 0.9012 - lr: 1.0000e-04\n",
      "Epoch 88/96\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.8036 - val_accuracy: 0.9050 - lr: 1.0000e-04\n",
      "Epoch 89/96\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.8205 - val_accuracy: 0.9013 - lr: 1.0000e-04\n",
      "Epoch 90/96\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8261 - val_accuracy: 0.9020 - lr: 1.0000e-04\n",
      "Epoch 91/96\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8284 - val_accuracy: 0.9011 - lr: 1.0000e-04\n",
      "Epoch 92/96\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.8298 - val_accuracy: 0.9019 - lr: 1.0000e-04\n",
      "Epoch 93/96\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.8231 - val_accuracy: 0.9021 - lr: 1.0000e-04\n",
      "Epoch 94/96\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.7999 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 95/96\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.90611\n",
      "200/200 - 1s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.8134 - val_accuracy: 0.9030 - lr: 1.0000e-04\n",
      "Epoch 96/96\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.90611\n",
      "200/200 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.8218 - val_accuracy: 0.9031 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(42)\n",
    "model = get_model()\n",
    "\n",
    "# Save initial weights\n",
    "model.save_weights('mlp.mnist.initial_weights.hdf5')\n",
    "history = model.fit(X_train, \n",
    "           y_train,\n",
    "           epochs=epochs, batch_size=batch_size, \n",
    "           validation_data = (X_val, y_val),\n",
    "           verbose=2, \n",
    "           callbacks=get_callbacks('mlp.mnist.first_train.hdf5')\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8217623829841614, 0.9031111001968384]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.374520480632782, 0.9061111211776733] [0.40363335609436035, 0.8996000289916992]\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('mlp.mnist.first_train.hdf5')\n",
    "print(model.evaluate(X_val, y_val, verbose=0), model.evaluate(X_test, y_test, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pm**(1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hidden_1', 'activation_3', 'hidden_2', 'activation_4', 'Salida', 'activation_5']\n"
     ]
    }
   ],
   "source": [
    "print([layer.name for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_sparse_model(filename, pruned_model_with_mask, pm = 0.20):\n",
    "    sparcity = 1 - pm\n",
    "    sprasity_sched = tfmot.sparsity.keras.ConstantSparsity(\n",
    "        sparcity, \n",
    "        0, # Do sparcity calculation in the first step\n",
    "        end_step=0, \n",
    "        frequency=10000000\n",
    "    )\n",
    "    model = get_model()\n",
    "    model.load_weights(filename)\n",
    "    prunned_model_layers = []\n",
    "    for i, layer in enumerate(pruned_model_with_mask.layers):\n",
    "        if isinstance(layer, tfmot.sparsity.keras.pruning_wrapper.PruneLowMagnitude):\n",
    "            l_weights = model.layers[i].get_weights()\n",
    "            l_weights[0] = l_weights[0]*layer.pruning_vars[0][1].numpy()\n",
    "            model.layers[i].set_weights(l_weights)        \n",
    "            prunned_model_layers.append(tfmot.sparsity.keras.prune_low_magnitude(model.layers[i], sprasity_sched))\n",
    "        else:\n",
    "            prunned_model_layers.append(model.layers[i])\n",
    "    prunned_model = Sequential(prunned_model_layers)\n",
    "    prunned_model.compile(optimizer=optimizers.SGD(lr=0), loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
    "    return prunned_model\n",
    "\n",
    "def get_prunned_model(filename, layers_to_pune, X_train, y_train, pm = 0.20):\n",
    "    \n",
    "    sparcity = 1 - pm\n",
    "    sprasity_sched = tfmot.sparsity.keras.ConstantSparsity(\n",
    "        sparcity, \n",
    "        0, # Do sparcity calculation in the first step\n",
    "        end_step=0, \n",
    "        frequency=10000000\n",
    "    )\n",
    "    model = get_model()\n",
    "    model.load_weights(filename)\n",
    "    prunned_model_layers = []\n",
    "    for layer in model.layers:\n",
    "        if layer.name in layers_to_pune:\n",
    "            prunned_model_layers.append(tfmot.sparsity.keras.prune_low_magnitude(layer, sprasity_sched))\n",
    "        else:\n",
    "            prunned_model_layers.append(layer)\n",
    "    pruned_model = Sequential(prunned_model_layers)\n",
    "    callbacks = [\n",
    "        tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "#         tfmot.sparsity.keras.PruningSummaries(log_dir='logs'),\n",
    "        ]\n",
    "    \n",
    "    # This is necesary to make keras calculate the mask, learning rate is 0\n",
    "    pruned_model.compile(optimizer=optimizers.SGD(lr=0), loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
    "    pruned_model.fit(X_train[0:1], y_train[0:1], epochs=1, batch_size=batch_size, verbose=0, callbacks=callbacks)\n",
    "    return pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/usuario/anaconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:199: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n"
     ]
    }
   ],
   "source": [
    "layers_to_pune = ['hidden_1', 'hidden_2', 'Salida']\n",
    "model_pruned_layers_trained = get_prunned_model('mlp.mnist.first_train.hdf5', layers_to_pune, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 871us/step - loss: 0.4274 - accuracy: 0.8540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.42743778228759766, 0.8539999723434448]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pruned_layers_trained.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 883us/step - loss: 0.4274 - accuracy: 0.8540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.42743778228759766, 0.8539999723434448]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_initialized = initialize_sparse_model('mlp.mnist.first_train.hdf5', model_pruned_layers_trained, )\n",
    "model_initialized.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_mask = model_pruned_layers_trained.get_layer('prune_low_magnitude_hidden_1').pruning_vars[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_low_magnitude_hidden_1: True, shape: (784, 1568), sparcity: 0.8000003253852561\n",
      "prune_low_magnitude_hidden_2: True, shape: (1568, 784), sparcity: 0.8000003253852561\n",
      "prune_low_magnitude_Salida: True, shape: (784, 10), sparcity: 0.8\n"
     ]
    }
   ],
   "source": [
    "def verify_mask_with_model_min_weights(model_, pruned_model):\n",
    "    if type(model_) == str:\n",
    "        model = get_model()\n",
    "        model.load_weights(model_)\n",
    "    else:\n",
    "        model = model_\n",
    "    for i, layer in enumerate(pruned_model.layers):\n",
    "        if isinstance(layer, tfmot.sparsity.keras.pruning_wrapper.PruneLowMagnitude):\n",
    "            weights_abs = np.abs(model.layers[i].get_weights()[0])\n",
    "            mask = layer.pruning_vars[0][1].numpy()\n",
    "            \n",
    "            # Verify that min of weights with mask 1 is higher than max of weights with mask 0\n",
    "            print(f'{layer.name}: {np.min(weights_abs[mask==1]) > np.max(weights_abs[mask==0])}, shape: {mask.shape}, sparcity: {1 - mask.sum()/np.product(mask.shape)}')\n",
    "            \n",
    "verify_mask_with_model_min_weights('mlp.mnist.first_train.hdf5', model_pruned_layers_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_low_magnitude_hidden_1: False, shape: (784, 1568), sparcity: 0.8000003253852561\n",
      "prune_low_magnitude_hidden_2: False, shape: (1568, 784), sparcity: 0.8000003253852561\n",
      "prune_low_magnitude_Salida: False, shape: (784, 10), sparcity: 0.8\n"
     ]
    }
   ],
   "source": [
    "verify_mask_with_model_min_weights('mlp.mnist.initial_weights.hdf5', model_pruned_layers_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize prunned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model = initialize_sparse_model('mlp.mnist.initial_weights.hdf5', model_pruned_layers_trained)\n",
    "compile_model(pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_low_magnitude_hidden_1: True, shape: (784, 1568), sparcity: 0.8000003253852561\n",
      "prune_low_magnitude_hidden_2: True, shape: (1568, 784), sparcity: 0.8000003253852561\n",
      "prune_low_magnitude_Salida: True, shape: (784, 10), sparcity: 0.8\n"
     ]
    }
   ],
   "source": [
    "verify_mask_with_model_min_weights(pruned_model, model_pruned_layers_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8000003253852561"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_index = 0\n",
    "(pruned_model.layers[l_index].get_weights()[0] == 0).sum()/np.product(model.layers[l_index].get_weights()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (pruned_model.layers[l_index].pruning_vars[0][1].numpy() == model_pruned_layers_trained.layers[l_index].pruning_vars[0][1].numpy()).sum()/np.product(model.layers[l_index].get_weights()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    ReduceLROnPlateau(monitor='val_accuracy', mode='max', factor=np.sqrt(0.1), patience=10, verbose=1),\n",
    "    ModelCheckpoint(filepath=f'mlp.mnist_no_kfold_sparse.hdf5', verbose=1, save_best_only=True, monitor='val_accuracy', mode='auto'),\n",
    "   # tfmot.sparsity.keras.PruningSummaries(log_dir='logs'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train prunned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/96\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.85711, saving model to mlp.mnist.sparse_train.hdf5\n",
      "200/200 - 1s - loss: 0.5636 - accuracy: 0.8264 - val_loss: 0.3956 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 2/96\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.85711 to 0.88311, saving model to mlp.mnist.sparse_train.hdf5\n",
      "200/200 - 1s - loss: 0.3312 - accuracy: 0.8794 - val_loss: 0.3293 - val_accuracy: 0.8831 - lr: 0.0010\n",
      "Epoch 3/96\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.88311 to 0.89167, saving model to mlp.mnist.sparse_train.hdf5\n",
      "200/200 - 1s - loss: 0.2799 - accuracy: 0.8974 - val_loss: 0.3099 - val_accuracy: 0.8917 - lr: 0.0010\n",
      "Epoch 4/96\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.89167\n",
      "200/200 - 1s - loss: 0.2506 - accuracy: 0.9074 - val_loss: 0.3120 - val_accuracy: 0.8871 - lr: 0.0010\n",
      "Epoch 5/96\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.89167 to 0.89322, saving model to mlp.mnist.sparse_train.hdf5\n",
      "200/200 - 1s - loss: 0.2263 - accuracy: 0.9162 - val_loss: 0.2958 - val_accuracy: 0.8932 - lr: 0.0010\n",
      "Epoch 6/96\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.89322 to 0.89356, saving model to mlp.mnist.sparse_train.hdf5\n",
      "200/200 - 1s - loss: 0.2089 - accuracy: 0.9220 - val_loss: 0.2980 - val_accuracy: 0.8936 - lr: 0.0010\n",
      "Epoch 7/96\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.89356 to 0.89533, saving model to mlp.mnist.sparse_train.hdf5\n",
      "200/200 - 1s - loss: 0.1915 - accuracy: 0.9295 - val_loss: 0.2896 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Epoch 8/96\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.89533 to 0.89556, saving model to mlp.mnist.sparse_train.hdf5\n",
      "200/200 - 1s - loss: 0.1788 - accuracy: 0.9328 - val_loss: 0.2991 - val_accuracy: 0.8956 - lr: 0.0010\n",
      "Epoch 9/96\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.89556 to 0.89956, saving model to mlp.mnist.sparse_train.hdf5\n",
      "200/200 - 1s - loss: 0.1657 - accuracy: 0.9383 - val_loss: 0.2917 - val_accuracy: 0.8996 - lr: 0.0010\n",
      "Epoch 10/96\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.89956\n",
      "200/200 - 1s - loss: 0.1546 - accuracy: 0.9420 - val_loss: 0.3103 - val_accuracy: 0.8989 - lr: 0.0010\n",
      "Epoch 11/96\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.89956\n",
      "200/200 - 1s - loss: 0.1426 - accuracy: 0.9470 - val_loss: 0.3019 - val_accuracy: 0.8982 - lr: 0.0010\n",
      "Epoch 12/96\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.89956\n",
      "200/200 - 1s - loss: 0.1352 - accuracy: 0.9494 - val_loss: 0.3188 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Epoch 13/96\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.89956\n",
      "200/200 - 1s - loss: 0.1242 - accuracy: 0.9536 - val_loss: 0.3122 - val_accuracy: 0.8968 - lr: 0.0010\n",
      "Epoch 14/96\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.89956\n",
      "200/200 - 1s - loss: 0.1162 - accuracy: 0.9568 - val_loss: 0.3134 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Epoch 15/96\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.89956\n",
      "200/200 - 1s - loss: 0.1078 - accuracy: 0.9594 - val_loss: 0.3298 - val_accuracy: 0.8983 - lr: 0.0010\n",
      "Epoch 16/96\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.89956\n",
      "200/200 - 1s - loss: 0.0994 - accuracy: 0.9628 - val_loss: 0.3402 - val_accuracy: 0.8990 - lr: 0.0010\n",
      "Epoch 17/96\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.89956 to 0.90167, saving model to mlp.mnist.sparse_train.hdf5\n",
      "200/200 - 1s - loss: 0.0940 - accuracy: 0.9644 - val_loss: 0.3472 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Epoch 18/96\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.90167\n",
      "200/200 - 1s - loss: 0.0901 - accuracy: 0.9659 - val_loss: 0.3567 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Epoch 19/96\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.90167\n",
      "200/200 - 1s - loss: 0.0824 - accuracy: 0.9691 - val_loss: 0.3766 - val_accuracy: 0.8996 - lr: 0.0010\n",
      "Epoch 20/96\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.90167\n",
      "200/200 - 1s - loss: 0.0785 - accuracy: 0.9709 - val_loss: 0.3742 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Epoch 21/96\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.90167\n",
      "200/200 - 1s - loss: 0.0755 - accuracy: 0.9715 - val_loss: 0.3909 - val_accuracy: 0.8976 - lr: 0.0010\n",
      "Epoch 22/96\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.90167\n",
      "200/200 - 1s - loss: 0.0718 - accuracy: 0.9730 - val_loss: 0.4033 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Epoch 23/96\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.90167\n",
      "200/200 - 1s - loss: 0.0660 - accuracy: 0.9755 - val_loss: 0.3972 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Epoch 24/96\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.90167\n",
      "200/200 - 1s - loss: 0.0600 - accuracy: 0.9778 - val_loss: 0.4067 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 25/96\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.90167\n",
      "200/200 - 1s - loss: 0.0612 - accuracy: 0.9775 - val_loss: 0.4329 - val_accuracy: 0.8944 - lr: 0.0010\n",
      "Epoch 26/96\n",
      "\n",
      "Epoch 00026: val_accuracy improved from 0.90167 to 0.90300, saving model to mlp.mnist.sparse_train.hdf5\n",
      "200/200 - 1s - loss: 0.0529 - accuracy: 0.9811 - val_loss: 0.4218 - val_accuracy: 0.9030 - lr: 0.0010\n",
      "Epoch 27/96\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.90300\n",
      "200/200 - 1s - loss: 0.0514 - accuracy: 0.9808 - val_loss: 0.4635 - val_accuracy: 0.8972 - lr: 0.0010\n",
      "Epoch 28/96\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.90300\n",
      "200/200 - 1s - loss: 0.0470 - accuracy: 0.9833 - val_loss: 0.4525 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Epoch 29/96\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.90300\n",
      "200/200 - 1s - loss: 0.0449 - accuracy: 0.9838 - val_loss: 0.4650 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Epoch 30/96\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.90300\n",
      "200/200 - 1s - loss: 0.0465 - accuracy: 0.9823 - val_loss: 0.4663 - val_accuracy: 0.8990 - lr: 0.0010\n",
      "Epoch 31/96\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.90300\n",
      "200/200 - 1s - loss: 0.0446 - accuracy: 0.9836 - val_loss: 0.4852 - val_accuracy: 0.8976 - lr: 0.0010\n",
      "Epoch 32/96\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.90300\n",
      "200/200 - 1s - loss: 0.0373 - accuracy: 0.9866 - val_loss: 0.4847 - val_accuracy: 0.9004 - lr: 0.0010\n",
      "Epoch 33/96\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.90300\n",
      "200/200 - 1s - loss: 0.0419 - accuracy: 0.9857 - val_loss: 0.5140 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Epoch 34/96\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.90300\n",
      "200/200 - 1s - loss: 0.0434 - accuracy: 0.9845 - val_loss: 0.4972 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Epoch 35/96\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.90300\n",
      "200/200 - 1s - loss: 0.0321 - accuracy: 0.9889 - val_loss: 0.5092 - val_accuracy: 0.9030 - lr: 0.0010\n",
      "Epoch 36/96\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00031622778103685084.\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.90300\n",
      "200/200 - 1s - loss: 0.0328 - accuracy: 0.9881 - val_loss: 0.5263 - val_accuracy: 0.8986 - lr: 0.0010\n",
      "Epoch 37/96\n",
      "\n",
      "Epoch 00037: val_accuracy improved from 0.90300 to 0.90400, saving model to mlp.mnist.sparse_train.hdf5\n",
      "200/200 - 1s - loss: 0.0163 - accuracy: 0.9957 - val_loss: 0.5162 - val_accuracy: 0.9040 - lr: 3.1623e-04\n",
      "Epoch 38/96\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.90400\n",
      "200/200 - 1s - loss: 0.0115 - accuracy: 0.9971 - val_loss: 0.5309 - val_accuracy: 0.9031 - lr: 3.1623e-04\n",
      "Epoch 39/96\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.90400\n",
      "200/200 - 1s - loss: 0.0107 - accuracy: 0.9973 - val_loss: 0.5416 - val_accuracy: 0.9022 - lr: 3.1623e-04\n",
      "Epoch 40/96\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.90400\n",
      "200/200 - 1s - loss: 0.0090 - accuracy: 0.9981 - val_loss: 0.5619 - val_accuracy: 0.9034 - lr: 3.1623e-04\n",
      "Epoch 41/96\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.90400\n",
      "200/200 - 1s - loss: 0.0087 - accuracy: 0.9982 - val_loss: 0.5652 - val_accuracy: 0.9032 - lr: 3.1623e-04\n",
      "Epoch 42/96\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.90400\n",
      "200/200 - 1s - loss: 0.0080 - accuracy: 0.9984 - val_loss: 0.5756 - val_accuracy: 0.9031 - lr: 3.1623e-04\n",
      "Epoch 43/96\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.90400\n",
      "200/200 - 1s - loss: 0.0075 - accuracy: 0.9984 - val_loss: 0.5840 - val_accuracy: 0.9028 - lr: 3.1623e-04\n",
      "Epoch 44/96\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.90400\n",
      "200/200 - 1s - loss: 0.0076 - accuracy: 0.9983 - val_loss: 0.5968 - val_accuracy: 0.9031 - lr: 3.1623e-04\n",
      "Epoch 45/96\n",
      "\n",
      "Epoch 00045: val_accuracy improved from 0.90400 to 0.90411, saving model to mlp.mnist.sparse_train.hdf5\n",
      "200/200 - 1s - loss: 0.0068 - accuracy: 0.9987 - val_loss: 0.5951 - val_accuracy: 0.9041 - lr: 3.1623e-04\n",
      "Epoch 46/96\n",
      "\n",
      "Epoch 00046: val_accuracy improved from 0.90411 to 0.90478, saving model to mlp.mnist.sparse_train.hdf5\n",
      "200/200 - 1s - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.6128 - val_accuracy: 0.9048 - lr: 3.1623e-04\n",
      "Epoch 47/96\n",
      "\n",
      "Epoch 00047: val_accuracy improved from 0.90478 to 0.90556, saving model to mlp.mnist.sparse_train.hdf5\n",
      "200/200 - 1s - loss: 0.0066 - accuracy: 0.9984 - val_loss: 0.6058 - val_accuracy: 0.9056 - lr: 3.1623e-04\n",
      "Epoch 48/96\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0059 - accuracy: 0.9989 - val_loss: 0.6222 - val_accuracy: 0.9034 - lr: 3.1623e-04\n",
      "Epoch 49/96\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0062 - accuracy: 0.9986 - val_loss: 0.6304 - val_accuracy: 0.9019 - lr: 3.1623e-04\n",
      "Epoch 50/96\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0056 - accuracy: 0.9989 - val_loss: 0.6554 - val_accuracy: 0.9036 - lr: 3.1623e-04\n",
      "Epoch 51/96\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0051 - accuracy: 0.9990 - val_loss: 0.6513 - val_accuracy: 0.9022 - lr: 3.1623e-04\n",
      "Epoch 52/96\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0043 - accuracy: 0.9993 - val_loss: 0.6625 - val_accuracy: 0.9022 - lr: 3.1623e-04\n",
      "Epoch 53/96\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0052 - accuracy: 0.9989 - val_loss: 0.6753 - val_accuracy: 0.9011 - lr: 3.1623e-04\n",
      "Epoch 54/96\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0060 - accuracy: 0.9986 - val_loss: 0.6633 - val_accuracy: 0.9022 - lr: 3.1623e-04\n",
      "Epoch 55/96\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0042 - accuracy: 0.9993 - val_loss: 0.6842 - val_accuracy: 0.9044 - lr: 3.1623e-04\n",
      "Epoch 56/96\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.6791 - val_accuracy: 0.9040 - lr: 3.1623e-04\n",
      "Epoch 57/96\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.00010000000639606199.\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.7005 - val_accuracy: 0.9038 - lr: 3.1623e-04\n",
      "Epoch 58/96\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0018 - accuracy: 0.9999 - val_loss: 0.6960 - val_accuracy: 0.9050 - lr: 1.0000e-04\n",
      "Epoch 59/96\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.7040 - val_accuracy: 0.9047 - lr: 1.0000e-04\n",
      "Epoch 60/96\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.7102 - val_accuracy: 0.9050 - lr: 1.0000e-04\n",
      "Epoch 61/96\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7142 - val_accuracy: 0.9048 - lr: 1.0000e-04\n",
      "Epoch 62/96\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7191 - val_accuracy: 0.9044 - lr: 1.0000e-04\n",
      "Epoch 63/96\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7228 - val_accuracy: 0.9047 - lr: 1.0000e-04\n",
      "Epoch 64/96\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7248 - val_accuracy: 0.9039 - lr: 1.0000e-04\n",
      "Epoch 65/96\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7292 - val_accuracy: 0.9050 - lr: 1.0000e-04\n",
      "Epoch 66/96\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7330 - val_accuracy: 0.9042 - lr: 1.0000e-04\n",
      "Epoch 67/96\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7411 - val_accuracy: 0.9038 - lr: 1.0000e-04\n",
      "Epoch 68/96\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.7449 - val_accuracy: 0.9048 - lr: 1.0000e-04\n",
      "Epoch 69/96\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 9.8442e-04 - accuracy: 1.0000 - val_loss: 0.7503 - val_accuracy: 0.9037 - lr: 1.0000e-04\n",
      "Epoch 70/96\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 9.5175e-04 - accuracy: 1.0000 - val_loss: 0.7526 - val_accuracy: 0.9047 - lr: 1.0000e-04\n",
      "Epoch 71/96\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 9.6132e-04 - accuracy: 1.0000 - val_loss: 0.7558 - val_accuracy: 0.9044 - lr: 1.0000e-04\n",
      "Epoch 72/96\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 9.2686e-04 - accuracy: 1.0000 - val_loss: 0.7593 - val_accuracy: 0.9047 - lr: 1.0000e-04\n",
      "Epoch 73/96\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 8.4051e-04 - accuracy: 1.0000 - val_loss: 0.7708 - val_accuracy: 0.9048 - lr: 1.0000e-04\n",
      "Epoch 74/96\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 8.3745e-04 - accuracy: 1.0000 - val_loss: 0.7728 - val_accuracy: 0.9053 - lr: 1.0000e-04\n",
      "Epoch 75/96\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 7.7938e-04 - accuracy: 1.0000 - val_loss: 0.7763 - val_accuracy: 0.9050 - lr: 1.0000e-04\n",
      "Epoch 76/96\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 7.3714e-04 - accuracy: 1.0000 - val_loss: 0.7870 - val_accuracy: 0.9049 - lr: 1.0000e-04\n",
      "Epoch 77/96\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 7.0747e-04 - accuracy: 1.0000 - val_loss: 0.7899 - val_accuracy: 0.9039 - lr: 1.0000e-04\n",
      "Epoch 78/96\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 6.2903e-04 - accuracy: 1.0000 - val_loss: 0.7953 - val_accuracy: 0.9044 - lr: 1.0000e-04\n",
      "Epoch 79/96\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 6.2208e-04 - accuracy: 1.0000 - val_loss: 0.8045 - val_accuracy: 0.9044 - lr: 1.0000e-04\n",
      "Epoch 80/96\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 7.2130e-04 - accuracy: 1.0000 - val_loss: 0.8045 - val_accuracy: 0.9046 - lr: 1.0000e-04\n",
      "Epoch 81/96\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 5.5573e-04 - accuracy: 1.0000 - val_loss: 0.8096 - val_accuracy: 0.9041 - lr: 1.0000e-04\n",
      "Epoch 82/96\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 7.2837e-04 - accuracy: 0.9999 - val_loss: 0.8154 - val_accuracy: 0.9037 - lr: 1.0000e-04\n",
      "Epoch 83/96\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 4.5841e-04 - accuracy: 1.0000 - val_loss: 0.8248 - val_accuracy: 0.9050 - lr: 1.0000e-04\n",
      "Epoch 84/96\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 4.6499e-04 - accuracy: 1.0000 - val_loss: 0.8268 - val_accuracy: 0.9051 - lr: 1.0000e-04\n",
      "Epoch 85/96\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 3.9867e-04 - accuracy: 1.0000 - val_loss: 0.8372 - val_accuracy: 0.9049 - lr: 1.0000e-04\n",
      "Epoch 86/96\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 3.9239e-04 - accuracy: 1.0000 - val_loss: 0.8433 - val_accuracy: 0.9049 - lr: 1.0000e-04\n",
      "Epoch 87/96\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.90556\n",
      "200/200 - 1s - loss: 4.0019e-04 - accuracy: 1.0000 - val_loss: 0.8546 - val_accuracy: 0.9043 - lr: 1.0000e-04\n",
      "Epoch 88/96\n",
      "\n",
      "Epoch 00088: val_accuracy improved from 0.90556 to 0.90567, saving model to mlp.mnist.sparse_train.hdf5\n",
      "200/200 - 1s - loss: 3.8735e-04 - accuracy: 1.0000 - val_loss: 0.8551 - val_accuracy: 0.9057 - lr: 1.0000e-04\n",
      "Epoch 89/96\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.90567\n",
      "200/200 - 1s - loss: 3.7526e-04 - accuracy: 1.0000 - val_loss: 0.8605 - val_accuracy: 0.9026 - lr: 1.0000e-04\n",
      "Epoch 90/96\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.90567\n",
      "200/200 - 1s - loss: 4.7133e-04 - accuracy: 1.0000 - val_loss: 0.8678 - val_accuracy: 0.9049 - lr: 1.0000e-04\n",
      "Epoch 91/96\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.90567\n",
      "200/200 - 1s - loss: 2.9864e-04 - accuracy: 1.0000 - val_loss: 0.8690 - val_accuracy: 0.9043 - lr: 1.0000e-04\n",
      "Epoch 92/96\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.90567\n",
      "200/200 - 1s - loss: 2.5666e-04 - accuracy: 1.0000 - val_loss: 0.8801 - val_accuracy: 0.9041 - lr: 1.0000e-04\n",
      "Epoch 93/96\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.90567\n",
      "200/200 - 1s - loss: 2.7400e-04 - accuracy: 1.0000 - val_loss: 0.8851 - val_accuracy: 0.9033 - lr: 1.0000e-04\n",
      "Epoch 94/96\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.90567\n",
      "200/200 - 1s - loss: 2.3316e-04 - accuracy: 1.0000 - val_loss: 0.8981 - val_accuracy: 0.9044 - lr: 1.0000e-04\n",
      "Epoch 95/96\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.90567\n",
      "200/200 - 1s - loss: 2.2667e-04 - accuracy: 1.0000 - val_loss: 0.8995 - val_accuracy: 0.9052 - lr: 1.0000e-04\n",
      "Epoch 96/96\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.90567\n",
      "200/200 - 1s - loss: 2.0908e-04 - accuracy: 1.0000 - val_loss: 0.9046 - val_accuracy: 0.9049 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history = pruned_model.fit(X_train, \n",
    "               y_train,\n",
    "               epochs=96, batch_size=batch_size, \n",
    "              validation_data = (X_val, y_val),\n",
    "               verbose=2, \n",
    "                    shuffle = True,\n",
    "                           callbacks=get_callbacks('mlp.mnist.sparse_train.hdf5') + [tfmot.sparsity.keras.UpdatePruningStep()]\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9045692086219788, 0.9048888683319092]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model.evaluate(X_val, y_val, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8550835251808167, 0.9056666493415833]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9800007939338684, 0.9009000062942505]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model.load_weights('mlp.mnist.sparse_train.hdf5')\n",
    "print(pruned_model.evaluate(X_val, y_val, verbose=0))\n",
    "pruned_model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check original mask equals prunned model mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_low_magnitude_hidden_1: True, shape: (784, 1568), sparcity: 0.8000003253852561\n",
      "prune_low_magnitude_hidden_2: True, shape: (1568, 784), sparcity: 0.8000003253852561\n",
      "prune_low_magnitude_Salida: True, shape: (784, 10), sparcity: 0.8\n"
     ]
    }
   ],
   "source": [
    "verify_mask_with_model_min_weights(pruned_model, model_pruned_layers_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 878us/step - loss: 0.4274 - accuracy: 0.8540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.42743778228759766, 0.8539999723434448]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pruned_layers_trained.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 956us/step - loss: 0.9800 - accuracy: 0.9009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9800007939338684, 0.9009000062942505]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
